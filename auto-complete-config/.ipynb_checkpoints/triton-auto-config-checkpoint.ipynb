{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4abfb16e-c5d3-4090-8ce2-4862c9d00739",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Triton Inference Server Auto-Complete-Config\n",
    "\n",
    "To simplify Triton Inference Server config.pbtxt you can utilize the Auto-Complete-Config feature to infer input/output shapes. In this case for the config.pbtxt we just include the platform and backend, add other parameters optionally if you would like. For this sample we'll take a Transformers Onnx model.\n",
    "\n",
    "## Setting\n",
    "\n",
    "For this sample we'll use a SageMaker Classic Notebook Instance, conda_py3 kernel and g5.4xlarge instance family."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4830601-196b-405b-91f6-fc9840a688fc",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec7ec6-a396-4447-b737-0d890b3d9803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c7e6da-295e-4dc4-b0f3-bb419478e980",
   "metadata": {},
   "source": [
    "## Local Inference & Onnx Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa8e48-7cfb-4eb4-873d-c10a7b8037e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-bert-base-dot-v5\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/msmarco-bert-base-dot-v5\")\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "query = \"How many people live in London?\"\n",
    "encoded_input = tokenizer(query, padding=True, truncation=True, return_tensors='pt')\n",
    "#print(encoded_input)\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input, return_dict=True)\n",
    "    #print(model_output)\n",
    "# Perform pooling\n",
    "embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "#embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d339d8-0a4c-41df-b625-fe75a9ee2f31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import transformers\n",
    "from transformers.onnx import FeaturesManager\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-bert-base-dot-v5\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/msmarco-bert-base-dot-v5\")\n",
    "\n",
    "# load config\n",
    "model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model)\n",
    "onnx_config = model_onnx_config(model.config)\n",
    "\n",
    "# export\n",
    "onnx_inputs, onnx_outputs = transformers.onnx.export(\n",
    "        preprocessor=tokenizer,\n",
    "        model=model,\n",
    "        config=onnx_config,\n",
    "        opset=13,\n",
    "        output=Path(\"model.onnx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2fe589-940d-4d7e-96be-33784b36fba5",
   "metadata": {},
   "source": [
    "## Triton Inference Server Setup\n",
    "\n",
    "Note we need to setup our model artifacts in a structure that Triton Inf Server expects:\n",
    "```\n",
    "- triton-serve-onnx\n",
    "    - sentence\n",
    "        - 1\n",
    "            - model.onnx\n",
    "        - config.pbtxt (adjusted for auto config)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d81a65-2b98-44f6-9a4e-ac6141b5741e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir triton-serve-onnx\n",
    "cd triton-serve-onnx\n",
    "mkdir sentence\n",
    "cd sentence\n",
    "touch config.pbtxt\n",
    "mkdir 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d671f57f-e169-46b7-ad80-c97118706164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile triton-serve-onnx/sentence/config.pbtxt\n",
    "name: \"sentence\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}\n",
    "dynamic_batching {\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9c5ec-ff2e-4adf-b882-1657e8a31b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv model.onnx triton-serve-onnx/sentence/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db2306b-a6ac-4469-a68c-94435926d080",
   "metadata": {},
   "source": [
    "## Sample Inference\n",
    "We can prepare the payload using the transformers tokenizer with the input formatted as needed for the model. We can then simply use the Python requests library or Triton Client for inference. \n",
    "\n",
    "Prior to inference ensure to start the container with the following command (adjust path and container as needed):\n",
    "\n",
    "```\n",
    "docker run --gpus=all --shm-size=4G --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/ec2-user/SageMaker/triton-serve-onnx:/model_repository nvcr.io/nvidia/tritonserver:23.12-py3 tritonserver --model-repository=/model_repository --exit-on-error=false --log-verbose=1 --strict-model-config=false\n",
    "```\n",
    "\n",
    "Note that we include the flag for strict model config being false as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df705a8d-c3be-4df5-98cd-c3a2415b8a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare client payload\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-bert-base-dot-v5\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokenized_text = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    payload = {}\n",
    "    payload[\"inputs\"] = []\n",
    "    payload[\"inputs\"].append(\n",
    "        {\n",
    "            \"name\": \"input_ids\",\n",
    "            \"shape\": tokenized_text.input_ids.shape,\n",
    "            \"datatype\": \"INT64\",\n",
    "            \"data\": tokenized_text.input_ids.tolist(),\n",
    "        }\n",
    "    )\n",
    "    payload[\"inputs\"].append(\n",
    "        {\n",
    "            \"name\": \"token_type_ids\",\n",
    "            \"shape\": tokenized_text.token_type_ids.shape,\n",
    "            \"datatype\": \"INT64\",\n",
    "            \"data\": tokenized_text.token_type_ids.tolist(),\n",
    "        }\n",
    "    )\n",
    "    payload[\"inputs\"].append(\n",
    "        {\n",
    "            \"name\": \"attention_mask\",\n",
    "            \"shape\": tokenized_text.attention_mask.shape,\n",
    "            \"datatype\": \"INT64\",\n",
    "            \"data\": tokenized_text.attention_mask.tolist(),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return payload\n",
    "sampPayload = tokenize_text([\"This is a test\"])\n",
    "sampPayload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9ac8c-9a90-416e-8cdc-0fc56ec39eca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Specify the model name and version\n",
    "model_name = \"sentence\" #specified in config.pbtxt\n",
    "model_version = \"1\"\n",
    "\n",
    "# Set the inference URL based on the Triton server's address\n",
    "url = f\"http://localhost:8000/v2/models/{model_name}/versions/{model_version}/infer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17831aa-d2ce-47f2-932a-728c2f6aa28d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample invoke onnx model\n",
    "response = requests.post(url, data=json.dumps(sampPayload))\n",
    "response.raise_for_status()\n",
    "\n",
    "# output result\n",
    "inference_result = response.json()\n",
    "print(inference_result['outputs'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
