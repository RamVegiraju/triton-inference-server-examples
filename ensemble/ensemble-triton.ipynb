{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a40451-6e5a-4152-89e5-886a2006ecbd",
   "metadata": {},
   "source": [
    "# Triton Ensembles\n",
    "In this example we will explore how we can stitch together multiple models using Triton Ensembles. With Triton Ensembles you can orchestrate complex workflows involving various models from the same or different backend engines. We recommend locally testing your logic before building out this pipeline. Here's a bit of a visual for how the architecture looks:\n",
    "\n",
    "![Arch diagram](triton-ensemble.jpg)\n",
    "\n",
    "## Directory Setup\n",
    "Triton also expects a certain folder structure to properly pick up on different artifacts, here's a high-level visual of how this should look for ensembles:\n",
    "\n",
    "```\n",
    "- hf_pipeline\n",
    "  - preprocess\n",
    "      - 1\n",
    "          - model.py\n",
    "      - config.pbtxt\n",
    "  - torch_classifier\n",
    "      - 1\n",
    "          - model.pt\n",
    "      - config.pbtxt\n",
    "  - postprocess\n",
    "      - 1\n",
    "          - model.py\n",
    "      - config.pbtxt\n",
    "  - text_ensemble \n",
    "      - 1\n",
    "          - stubbed file (need for ensemble to be picked upon)\n",
    "      - config.pbtxt\n",
    "```\n",
    "\n",
    "## Notebook Setup\n",
    "Using a SageMaker g5.4xlarge Classic Notebook Instance, you can use any environment just ensure it comes with Docker installed/setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2de666-fe00-472a-83a7-4bfc0b9b9dd7",
   "metadata": {},
   "source": [
    "## Generate Torch Model Artifact\n",
    "This is how we generated the model.pt in the torch_classifier directory, we do not recommend uploading the artifact like we did here, please run with your own model and don't share on Git especially if it's a custom one you are keeping private."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61600d43-04aa-43b3-8d05-edeeea81e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "class HFWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return out.logits  # <-- return a Tensor, not a dict\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "    model.eval()\n",
    "\n",
    "    wrapped = HFWrapper(model)\n",
    "\n",
    "    example_input_ids = torch.randint(0, 100, (1, 128))\n",
    "    example_attention_mask = torch.ones((1, 128), dtype=torch.long)\n",
    "\n",
    "    traced = torch.jit.trace(\n",
    "        wrapped,\n",
    "        (example_input_ids, example_attention_mask)\n",
    "    )\n",
    "    traced.save(\"model.pt\")\n",
    "    print(\"saved model.pt\")\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e614d9e2-f2c3-4ef0-9962-2251bd3286c9",
   "metadata": {},
   "source": [
    "## Build Custom Docker Image\n",
    "Traditionally you can just use the provided Triton Docker container, but we install some custom dependencies for the Python backend. We extend the base Triton image and install transformers:\n",
    "\n",
    "```\n",
    "FROM nvcr.io/nvidia/tritonserver:25.10-py3\n",
    "\n",
    "# install what your python model needs\n",
    "RUN pip install --no-cache-dir transformers torch\n",
    "```\n",
    "\n",
    "Build Docker image:\n",
    "```\n",
    "docker build -t custom-triton .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33c8bc-f502-4bf1-bec8-75063cf1e4f4",
   "metadata": {},
   "source": [
    "## Start Docker Container\n",
    "Point towards the path of your model repository artifacts and the image built:\n",
    "\n",
    "```\n",
    "docker run --gpus=all --shm-size=4G --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/ec2-user/SageMaker/hf_pipeline:/model_repository custom-triton:latest tritonserver --model-repository=/model_repository --exit-on-error=false --log-verbose=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87910c44-80f4-4e3b-bada-48f09bd2810e",
   "metadata": {},
   "source": [
    "## Sample Model Inference\n",
    "Can use the Triton Python Client library: https://github.com/triton-inference-server/client/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1865b3e-e8ab-4e6f-9227-d0087220786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "# Connect to Triton (adjust host:port if running remotely)\n",
    "client = httpclient.InferenceServerClient(\"localhost:8000\")\n",
    "\n",
    "# Prepare your text inputs\n",
    "texts = [\n",
    "    \"I am SO UPSET!\",\n",
    "    \"I am super happy!!\",\n",
    "    \"Life is great!\"\n",
    "]\n",
    "\n",
    "# Triton expects BYTES tensors for string inputs\n",
    "input_data = np.array([t.encode(\"utf-8\") for t in texts], dtype=object)\n",
    "\n",
    "# Create an InferInput matching the ensembleâ€™s config.pbtxt\n",
    "infer_input = httpclient.InferInput(\"TEXT\", [len(texts)], \"BYTES\")\n",
    "infer_input.set_data_from_numpy(input_data)\n",
    "\n",
    "# Perform inference on your ensemble\n",
    "response = client.infer(\n",
    "    model_name=\"text_ensemble\",\n",
    "    inputs=[infer_input]\n",
    ")\n",
    "\n",
    "# Retrieve outputs\n",
    "#   - If your ensemble outputs LABEL (postprocess stage)\n",
    "#   - Or LOGITS if you skipped postprocess\n",
    "try:\n",
    "    labels = response.as_numpy(\"LABEL\")\n",
    "    print([l.decode(\"utf-8\") for l in labels])\n",
    "except KeyError:\n",
    "    logits = response.as_numpy(\"LOGITS\")\n",
    "    print(\"Raw logits:\\n\", logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
