{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7a9717-e8af-424d-bbf4-d501741eb275",
   "metadata": {},
   "source": [
    "## Triton Inference Server PyTorch Example\n",
    "\n",
    "- <b>References/Docs</b>: https://pytorch.org/TensorRT/tutorials/serving_torch_tensorrt_with_triton.html\n",
    "- <b>Environment/Setup</b>: SageMaker g4dn.xlarge classic notebook instance, conda_pytorch_p310 kernel. You can also run this model on a CPU instance if you desire, just using GPU for the entirety of this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d33bc21-0606-4e3a-b6f9-223e9d564d58",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We will be orchestrating inference with the HTTP Triton Client: https://github.com/triton-inference-server/client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eaa775-f145-48d7-a381-2b890d412030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tritonclient[http]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d8f01b-129d-4005-834a-db717ec895e2",
   "metadata": {},
   "source": [
    "### Dummy Local TorchScript Model\n",
    "\n",
    "Credits: Utilized ChatGPT to give me a mock simple linear regression PyTorch model, just so we have a model artifact to work with. In this case it will be torchsript model (model.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e076758-15c1-4828-985d-bbdae39460fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623aa480-788f-4078-ac19-22af99df8f88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate some random data for a linear regression problem\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 1 + 2 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Convert the NumPy arrays to PyTorch tensors\n",
    "X_tensor = torch.from_numpy(X).float()\n",
    "y_tensor = torch.from_numpy(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33992c3-50a2-4d11-ba37-284609ae0db3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a linear regression model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # One input feature, one output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Instantiate the model and specify a loss function and optimizer\n",
    "model = LinearRegression()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f059667a-5d43-4191-abd1-b5a7ce79d258",
   "metadata": {},
   "source": [
    "### Save Model + Local Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9aa5ad-bc49-4811-95d2-b553c3f48edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model as a torchscript model\n",
    "torch.jit.save(torch.jit.script(model), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef20b16-6fbc-438b-92b1-458cc57468ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = torch.jit.load('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b974990-f1ba-4ca8-9cec-e5706cde4f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample inference\n",
    "test = torch.tensor([[2.5]])\n",
    "pred = loaded_model(test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c16b9f-bf73-405e-8349-41c465251ff8",
   "metadata": {},
   "source": [
    "### Triton Setup\n",
    "\n",
    "We first setup the artifacts we need in the structure the model server expects, this is the model repository structure it's expecting for this backend:\n",
    "\n",
    "- linear_regression_model\n",
    "    - 1\n",
    "        - model.pt\n",
    "        - model.py (optional, not included in this case)\n",
    "    - config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38de3cd-cc1c-4d19-b039-a8ba9a6cab8f",
   "metadata": {},
   "source": [
    "#### Create Config File For PyTorch Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585b0545-9d3c-45b5-aae7-61d6142cf422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile config.pbtxt\n",
    "name: \"linear_regression_model\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "\n",
    "input {\n",
    "  name: \"input\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: [ 1, 1 ]\n",
    "}\n",
    "\n",
    "output {\n",
    "  name: \"output\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: [ 1, 1 ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0870975-8360-40e2-b3ae-5dcd7d90da6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir linear_regression_model\n",
    "mv config.pbtxt model.pt linear_regression_model\n",
    "cd linear_regression_model\n",
    "mkdir 1\n",
    "mv model.pt 1/\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2bc61-4e4a-4b2d-b9e1-5fd781781876",
   "metadata": {},
   "source": [
    "Second we want to run the following Docker command in a terminal to ensure we have Triton Inference Server up and running, we use the latest Triton Image available to execute it (updated to 25.03). Ensure to update the command to reflect the path for where you are executing this (run a pwd command where this NB is located).\n",
    "\n",
    "```\n",
    "docker run --gpus all --rm -p 8000:8000 -p 8001:8001 -p 8002:8002 -v /home/ec2-user/SageMaker/triton-inference-server-examples/pytorch-backend:/models nvcr.io/nvidia/tritonserver:25.03-py3 tritonserver --model-repository=/models --exit-on-error=false --log-verbose=1\n",
    "```\n",
    "\n",
    "Once the server is started we can send requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ce018-9c4c-4ec6-8a48-d6c7276bf69b",
   "metadata": {},
   "source": [
    "### Triton Inference\n",
    "\n",
    "There's two different ways we can run inference\n",
    "\n",
    "1. Using Python requests library and passing in the Triton Server at port 8000 for HTTP requests\n",
    "2. Utilizing Triton Client Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2885c74c-450a-48f3-9df0-56a6040f92df",
   "metadata": {},
   "source": [
    "#### Python Requests Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead642fb-3f8b-4645-a2da-3d8c5770488f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# sample data\n",
    "input_data = np.array([[2.5]], dtype=np.float32)\n",
    "\n",
    "# Specify the model name and version\n",
    "model_name = \"linear_regression_model\" #specified in config.pbtxt\n",
    "model_version = \"1\"\n",
    "\n",
    "# Set the inference URL based on the Triton server's address\n",
    "url = f\"http://localhost:8000/v2/models/{model_name}/versions/{model_version}/infer\"\n",
    "\n",
    "# payload with input params\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"input\",  # what you named input in config.pbtxt\n",
    "            \"datatype\": \"FP32\",  \n",
    "            \"shape\": input_data.shape,\n",
    "            \"data\": input_data.tolist(),\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# sample invoke\n",
    "response = requests.post(url, data=json.dumps(payload))\n",
    "response.raise_for_status()\n",
    "\n",
    "# output result\n",
    "inference_result = response.json()\n",
    "output_data = np.array(inference_result[\"outputs\"][0][\"data\"])\n",
    "output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd3a14-7526-48db-88a8-35d6cf66c9f3",
   "metadata": {},
   "source": [
    "#### Triton Client Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924db29-51ef-45f9-b1db-1e82559ed2b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "# setup triton inference client\n",
    "client = httpclient.InferenceServerClient(url=\"localhost:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc432a-b81e-4b22-b486-e773131ba3f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# triton can infer the inputs from your config values\n",
    "inputs = httpclient.InferInput(\"input\", input_data.shape, datatype=\"FP32\")\n",
    "inputs.set_data_from_numpy(input_data) #we set a numpy array in this case\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecdfe7b-f30f-4c52-adde-a60ef303234d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output configuration\n",
    "outputs = httpclient.InferRequestedOutput(\"output\")\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb72becc-bbf2-4e55-887e-319372cb96a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sample inference\n",
    "res = client.infer(model_name = \"linear_regression_model\", inputs=[inputs], outputs=[outputs],\n",
    "                  )\n",
    "inference_output = res.as_numpy('output') #serialize numpy output\n",
    "inference_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6ca61-38f1-4889-8f96-207a6eb2060a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(100):\n",
    "    res = client.infer(model_name = \"linear_regression_model\", inputs=[inputs], outputs=[outputs],\n",
    "                  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
