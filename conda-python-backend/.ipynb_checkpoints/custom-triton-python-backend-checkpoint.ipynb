{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46886e17-c3c3-441b-997f-2ae39d9e1783",
   "metadata": {},
   "source": [
    "# Triton Inference Server Custom Python Backend Setup\n",
    "\n",
    "Traditionally for the Triton Inference Server, the Python backend can be used to customize your server with additional requirements (ex: transformers). Generally a conda-pack environment is expected to be packaged in as displayed in this blog: https://aws.amazon.com/blogs/machine-learning/host-ml-models-on-amazon-sagemaker-using-triton-python-backend/. However, this can get heavy having such large artifacts, thus we explore installing these dependencies at the container level itself in this sample.\n",
    "\n",
    "### Setting\n",
    "For this example we are working in a SageMaker Classic Notebook Instance (g5.4xlarge), increase the Notebook Volume as needed, the conda-pack and Docker operations will be heavy.\n",
    "\n",
    "### Credits/Reference\n",
    "- SageMaker Triton BYOC Example: https://github.com/aws-samples/sagemaker-hosting/tree/main/Inference-Serving-Options/SageMaker-Triton/Triton-BYOC\n",
    "- SageMaker Python Backend Blog: https://aws.amazon.com/blogs/machine-learning/host-ml-models-on-amazon-sagemaker-using-triton-python-backend/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142e15a-14c8-4e87-b0f0-2306bee699ed",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bdb9d3-af19-4f3f-9b8d-17ef87df0c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]\n",
    "!pip install -U sagemaker numpy transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0710779-a2ec-4fa6-b429-9006ec97b9f3",
   "metadata": {},
   "source": [
    "## Building Docker Image & Setting Up Triton\n",
    "\n",
    "For the Python backend we will extend the latest Triton Server Docker container and install the necessary dependencies (torch, transformers in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680fb0b-0870-4a6f-825a-56dd70830723",
   "metadata": {},
   "source": [
    "### Build Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c5f6d-4b97-401f-a3c0-2f7c4c6cb05b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM nvcr.io/nvidia/tritonserver:25.10-py3\n",
    "\n",
    "# install what your python model needs\n",
    "RUN pip install --no-cache-dir transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee74d01-d3be-4462-830f-b13dd27e8dfe",
   "metadata": {},
   "source": [
    "#### Docker Build\n",
    "Run the following command in the Terminal/CLI:\n",
    "\n",
    "```\n",
    "docker build -t custom-triton .\n",
    "```\n",
    "\n",
    "To check the image has been built, use the following command:\n",
    "\n",
    "```\n",
    "docker images\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ca0ad-2e04-4c4d-a088-2d1c6ca6ca58",
   "metadata": {},
   "source": [
    "## Local Model Inference\n",
    "\n",
    "For this sample, let's use a BERT transformers model, here's a local Python code inference with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909721ee-6317-4050-a442-72b194977d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "model = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "output = model(\"I am super happy\")\n",
    "res = np.array(output)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6c4a4-e1b2-4bfd-91f0-8a539aad4dc8",
   "metadata": {},
   "source": [
    "## Triton Artifact Creation\n",
    "\n",
    "Ensure the following structure is present within the repository:\n",
    "\n",
    "```\n",
    "- nlp\n",
    "    - sentiment (Triton model name)\n",
    "        - 1\n",
    "            - model.py (inference logic)\n",
    "        - config.pbtxt (input/output specs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a76b22d-1c1c-4a18-92c7-bd39db2f5dc0",
   "metadata": {},
   "source": [
    "## Docker Container Startup\n",
    "\n",
    "To startup the docker container run this command via the terminal or a shell script:\n",
    "\n",
    "```\n",
    "docker run --gpus=all --shm-size=4G --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/ec2-user/SageMaker/nlp:/model_repository custom-triton:latest tritonserver --model-repository=/model_repository --exit-on-error=false --log-verbose=1\n",
    "```\n",
    "\n",
    "Note run this command in the same root directory as the notebook, in this case we are using the SageMaker Notebook Instance home directory path adjust it to your appropriate directory. We also use the \"custom-triton\" image we built, change this if you named your Docker image otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9335a0fd-7447-45e8-ae22-84e0c67a8fe7",
   "metadata": {},
   "source": [
    "## Sample Inference\n",
    "\n",
    "We can use the Triton Client library or Python requests library for sample inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89fb5d-6d3c-45c3-a79a-bb71c0f6133e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tritonclient.http as http_client\n",
    "triton_client = http_client.InferenceServerClient(url=\"localhost:8000\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8fb2d9-fec3-4d40-9ed4-ce7c108997e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json \n",
    "# Create inputs to send to Triton\n",
    "model_name = \"sentiment\"\n",
    "text_inputs = [\"I am super happy right now\"]\n",
    "\n",
    "# structure inputs\n",
    "inputs = []\n",
    "inputs.append(http_client.InferInput(\"text\", [1], \"BYTES\"))\n",
    "input0_real = np.array(text_inputs, dtype=np.object_)\n",
    "inputs[0].set_data_from_numpy(input0_real)\n",
    "\n",
    "# structure outputs\n",
    "outputs = []\n",
    "outputs.append(http_client.InferRequestedOutput(\"sent_arr\"))\n",
    "\n",
    "# sample inference\n",
    "results = triton_client.infer(model_name=model_name, inputs=inputs, outputs=outputs)\n",
    "sentiment = results.as_numpy('sent_arr')\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275ea42-e3ed-45d0-804f-e5b4a9cfc097",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(100):\n",
    "    results = triton_client.infer(model_name=model_name, inputs=inputs, outputs=outputs)\n",
    "    sentiment = results.as_numpy('sent_arr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
