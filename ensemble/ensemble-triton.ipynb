{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba5f79b-7b0f-4f38-b3c0-256fb88aab2c",
   "metadata": {},
   "source": [
    "# Triton Inference Server Ensemble Invoke\n",
    "\n",
    "In this notebook we'll explore how to use Triton Ensemble mode to stitch together multiple models for inference, in this case we will take a sample embeddings model and show how we can use the tokenizer (python backend) and embeddings model (onnx backend) for inference in ensemble mode.\n",
    "\n",
    "## Prerequisites\n",
    "- Ensure you have ran onnx-exporter.ipynb to create the model.onnx, the file is also in the repository in the model repository structure.\n",
    "- Ensure you have created the custom Docker image with transformers installed at runtime. Steps are in the README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caaa13e-14a9-456b-b771-8e7f9425b9f8",
   "metadata": {},
   "source": [
    "## Client Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e1453b-a2a1-4324-b483-836c233bf7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nvidia-pyindex\n",
    "#!pip install tritonclient[http]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eb573e-5067-4164-b54a-af1633747b46",
   "metadata": {},
   "source": [
    "## Start Container\n",
    "\n",
    "Use the following command to startup the Docker container in the CLI, ensure you have built this image following the steps in the README.md\n",
    "\n",
    "```\n",
    "docker run --gpus=all --shm-size=4G --rm -p8000:8000 -p8001:8001 -p8002:8002 -v$(pwd)/hf_pipeline:/model_repository custom-triton:latest tritonserver --model-repository=/model_repository --exit-on-error=false --log-verbose=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c4c3d-5c0d-4208-8a7f-8d5c299a2e7d",
   "metadata": {},
   "source": [
    "## Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5948143-af06-40ca-8b78-b03fdcbf3477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tritonclient.http as http_client\n",
    "triton_client = http_client.InferenceServerClient(url=\"localhost:8000\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e8f8503-2c4b-489b-af64-084edd7d43f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/models/ensemble/infer, headers {'Inference-Header-Content-Length': 173}\n",
      "b'{\"inputs\":[{\"name\":\"INPUT0\",\"shape\":[1],\"datatype\":\"BYTES\",\"parameters\":{\"binary_data_size\":27}}],\"outputs\":[{\"name\":\"last_hidden_state\",\"parameters\":{\"binary_data\":true}}]}\\x17\\x00\\x00\\x00This is the test string'\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/octet-stream', 'inference-header-content-length': '237', 'content-length': '21741'}>\n",
      "bytearray(b'{\"model_name\":\"ensemble\",\"model_version\":\"1\",\"parameters\":{\"sequence_id\":0,\"sequence_start\":false,\"sequence_end\":false},\"outputs\":[{\"name\":\"last_hidden_state\",\"datatype\":\"FP32\",\"shape\":[1,7,768],\"parameters\":{\"binary_data_size\":21504}}]}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tritonclient.http._infer_result.InferResult at 0x7fdd6b8d77c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create inputs to send to Triton\n",
    "model_name = \"ensemble\"\n",
    "text_inputs = [\"This is the test string\"]\n",
    "\n",
    "# Text is passed to Trtion as BYTES\n",
    "inputs = []\n",
    "inputs.append(http_client.InferInput(\"INPUT0\", [1], \"BYTES\"))\n",
    "input0_real = np.array(text_inputs, dtype=np.object_)\n",
    "inputs[0].set_data_from_numpy(input0_real)\n",
    "\n",
    "outputs = []\n",
    "outputs.append(http_client.InferRequestedOutput(\"last_hidden_state\"))\n",
    "\n",
    "results = triton_client.infer(model_name=model_name, inputs=inputs, outputs=outputs)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ad856a-ff47-41db-ba9d-5b337801bc61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.36872923, -0.21283835,  0.5032521 , ...,  0.20257561,\n",
       "         -0.14473006,  0.16335659],\n",
       "        [ 0.16864908, -0.29112825,  0.43506908, ...,  0.14554416,\n",
       "          0.04653071,  0.18665251],\n",
       "        [ 0.02562865, -0.32240435,  0.40868905, ...,  0.10257501,\n",
       "          0.07761161,  0.35088107],\n",
       "        ...,\n",
       "        [ 0.41528454, -0.3950774 ,  0.28445122, ...,  0.26427785,\n",
       "          0.18659928, -0.6684136 ],\n",
       "        [ 1.3407743 , -0.2955229 ,  0.2634831 , ...,  0.33415437,\n",
       "          0.00846357, -0.1535036 ],\n",
       "        [ 0.10819956,  0.10353471,  0.18187995, ...,  0.37455615,\n",
       "          0.08028258, -0.06970064]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.as_numpy('last_hidden_state')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
