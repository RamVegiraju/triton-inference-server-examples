{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46886e17-c3c3-441b-997f-2ae39d9e1783",
   "metadata": {},
   "source": [
    "# Triton Inference Server Custom Python Backend Setup\n",
    "\n",
    "Traditionally for the Triton Inference Server, the Python backend can be used to customize your server with additional requirements (ex: transformers). Generally a conda-pack environment is expected to be packaged in as displayed in this blog: https://aws.amazon.com/blogs/machine-learning/host-ml-models-on-amazon-sagemaker-using-triton-python-backend/. However, this can get heavy having such large artifacts, thus we explore installing these dependencies at the container level itself in this sample.\n",
    "\n",
    "### Setting\n",
    "For this example we are working in a SageMaker Classic Notebook Instance (g5.4xlarge), increase the Notebook Volume as needed, the conda-pack and Docker operations will be heavy.\n",
    "\n",
    "### Credits/Reference\n",
    "- SageMaker Triton BYOC Example: https://github.com/aws-samples/sagemaker-hosting/tree/main/Inference-Serving-Options/SageMaker-Triton/Triton-BYOC\n",
    "- SageMaker Python Backend Blog: https://aws.amazon.com/blogs/machine-learning/host-ml-models-on-amazon-sagemaker-using-triton-python-backend/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142e15a-14c8-4e87-b0f0-2306bee699ed",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70bdb9d3-af19-4f3f-9b8d-17ef87df0c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install nvidia-pyindex\n",
    "#!pip install tritonclient[http]\n",
    "#!pip install -U sagemaker numpy transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0710779-a2ec-4fa6-b429-9006ec97b9f3",
   "metadata": {},
   "source": [
    "## Building Docker Image & Setting Up Triton\n",
    "\n",
    "For the Python backend we will create a conda environment using conda-pack with the necessary dependencies and point towards this in our Dockerfile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f151ea5-ee19-482c-939c-38777bb765b1",
   "metadata": {},
   "source": [
    "### Conda Env Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310bf028-41a4-4d67-a89d-c71737ef8281",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting conda_dependencies.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile conda_dependencies.sh\n",
    "conda create -y -n transformers_env python=3.10\n",
    "source ~/anaconda3/etc/profile.d/conda.sh\n",
    "source activate transformers_env\n",
    "export PYTHONNOUSERSITE=True\n",
    "pip install torch --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "pip install transformers\n",
    "pip install conda-pack\n",
    "conda-pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8654926-33ce-4001-a2ed-e1fa54afd4c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!bash conda_dependencies.sh # create the conda-pack env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680fb0b-0870-4a6f-825a-56dd70830723",
   "metadata": {},
   "source": [
    "### Build Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341c5f6d-4b97-401f-a3c0-2f7c4c6cb05b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM nvcr.io/nvidia/tritonserver:23.12-py3\n",
    "\n",
    "#Install any additional libraries\n",
    "RUN echo \"Adding conda package to Docker image\"\n",
    "RUN mkdir -p /home/condpackenv/\n",
    "\n",
    "# Copy conda env\n",
    "COPY transformers_env.tar.gz /home/condpackenv/transformers_env.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee74d01-d3be-4462-830f-b13dd27e8dfe",
   "metadata": {},
   "source": [
    "#### Docker Build\n",
    "Run the following command in the Terminal/CLI:\n",
    "\n",
    "```\n",
    "docker build -t custom-triton .\n",
    "```\n",
    "\n",
    "To check the image has been built, use the following command:\n",
    "\n",
    "```\n",
    "docker images\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ca0ad-2e04-4c4d-a088-2d1c6ca6ca58",
   "metadata": {},
   "source": [
    "## Local Model Inference\n",
    "\n",
    "For this sample, let's use a BERT transformers model, here's a local Python code inference with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "909721ee-6317-4050-a442-72b194977d68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '5 stars', 'score': 0.8162010908126831}]\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "model = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "output = model(\"I am super happy\")\n",
    "res = np.array(output)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6c4a4-e1b2-4bfd-91f0-8a539aad4dc8",
   "metadata": {},
   "source": [
    "## Triton Artifact Creation\n",
    "\n",
    "Ensure the following structure is present within the repository:\n",
    "\n",
    "```\n",
    "- nlp\n",
    "    - sentiment (Triton model name)\n",
    "        - 1\n",
    "            - model.py (inference logic)\n",
    "        - config.pbtxt (input/output specs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a76b22d-1c1c-4a18-92c7-bd39db2f5dc0",
   "metadata": {},
   "source": [
    "## Docker Container Startup\n",
    "\n",
    "To startup the docker container run this command via the terminal or a shell script:\n",
    "\n",
    "```\n",
    "docker run --gpus=all --shm-size=4G --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/ec2-user/SageMaker/nlp:/model_repository custom-triton:latest tritonserver --model-repository=/model_repository --exit-on-error=false --log-verbose=1\n",
    "```\n",
    "\n",
    "Note run this command in the same root directory as the notebook, in this case we are using the SageMaker Notebook Instance home directory path adjust it to your appropriate directory. We also use the \"custom-triton\" image we built, change this if you named your Docker image otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9335a0fd-7447-45e8-ae22-84e0c67a8fe7",
   "metadata": {},
   "source": [
    "## Sample Inference\n",
    "\n",
    "We can use the Triton Client library or Python requests library for sample inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a89fb5d-6d3c-45c3-a79a-bb71c0f6133e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tritonclient.http as http_client\n",
    "triton_client = http_client.InferenceServerClient(url=\"localhost:8000\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d8fb2d9-fec3-4d40-9ed4-ce7c108997e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/models/sentiment/infer, headers {'Inference-Header-Content-Length': 162}\n",
      "b'{\"inputs\":[{\"name\":\"text\",\"shape\":[1],\"datatype\":\"BYTES\",\"parameters\":{\"binary_data_size\":30}}],\"outputs\":[{\"name\":\"sent_arr\",\"parameters\":{\"binary_data\":true}}]}\\x1a\\x00\\x00\\x00I am super happy right now'\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/octet-stream', 'inference-header-content-length': '146', 'content-length': '199'}>\n",
      "bytearray(b'{\"model_name\":\"sentiment\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"sent_arr\",\"datatype\":\"BYTES\",\"shape\":[1],\"parameters\":{\"binary_data_size\":53}}]}')\n",
      "[b\"{'label': '5 stars', 'score': 0.8433139324188232}\"]\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "# Create inputs to send to Triton\n",
    "model_name = \"sentiment\"\n",
    "text_inputs = [\"I am super happy right now\"]\n",
    "\n",
    "# structure inputs\n",
    "inputs = []\n",
    "inputs.append(http_client.InferInput(\"text\", [1], \"BYTES\"))\n",
    "input0_real = np.array(text_inputs, dtype=np.object_)\n",
    "inputs[0].set_data_from_numpy(input0_real)\n",
    "\n",
    "# structure outputs\n",
    "outputs = []\n",
    "outputs.append(http_client.InferRequestedOutput(\"sent_arr\"))\n",
    "\n",
    "# sample inference\n",
    "results = triton_client.infer(model_name=model_name, inputs=inputs, outputs=outputs)\n",
    "sentiment = results.as_numpy('sent_arr')\n",
    "print(sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
